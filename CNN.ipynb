{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afac71d-d74e-430f-ab63-09dc376e0b3e",
   "metadata": {},
   "source": [
    "## Red Neuronal Convolucional\n",
    "\n",
    "La arquitectura de esta red será de dos capas convolucionales, después de la primera se hace una operación de Pooling (MaxPool aunque se puede modificar por AvgPool, al igual que el tamaño del movimiento). Finalmente se pasará a una porción totalmente conexa de 4 capas. La función de activación será RELU para todas las capas. No se agregan valores de dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07377c43-d792-4499-8f2e-0f3b21bbb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fea8242-d219-4d49-b5ed-93f51b773cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    #En el constructor definimos la arquitectura\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5) \n",
    "        self.pool1 = nn.MaxPool2d(2,2) #Filtro MaxPool de tamaño 2x2, recorre bloques unitarios\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        \n",
    "        #self.pool2 = nn.AvgPool2d(3, stride=2) #Filtro AvgPool de tamaño 3x3, recorriendo bloques de dos en dos\n",
    "        # Esta capa se puede agregar pero se tienen que recalcular los valores de salida.\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 69)\n",
    "        self.fc4 = nn.Linear(69, 20)\n",
    "        self.fc5 = nn.Linear(20, 10) #Es una red para clasificación de 10 clases (ahorita veremos el dataset en cuestión)\n",
    "\n",
    "    \n",
    "    #En esta función definimos las funciones de activación entre cada capa, todas son RELU\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        #x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Inicializamos la red\n",
    "red_cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b50a6c8e-74b6-460d-9389-233ab8c2f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=69, bias=True)\n",
       "  (fc4): Linear(in_features=69, out_features=20, bias=True)\n",
       "  (fc5): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405796e-d438-47f4-917f-e42f7e33be82",
   "metadata": {},
   "source": [
    "### Funciones de pérdida\n",
    "\n",
    "Al tener un dataset donde hay 10 clases para clasificar, usaremos CrossEntropy (CrossEntropyLoss) como función de pérdida. De igual manera usaremos el optimizador adam con una razón de aprendizaje de 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f0db900-67b5-49b0-89cc-65e21632b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optim1 = optim.SGD(red_cnn.parameters(), lr=0.0001, momentum = 0.9)\n",
    "#optimizador = optim.Adam(red_cnn.parameters(), lr = 0.003, betas = (0.9,0.999), eps= 1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d4c8e-24cf-4a10-93a6-7328f9bb3084",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Antes de implementar el entrenamiento tenemos que tener información de entrenamiento, usaremos datasets extraídos de pytorch mismo. Para el entrenamiento haremos una iteración en 4 épocas siguiendo los lineamientos de la documentación de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf66bc13-b576-4460-9e50-0b18c9ba9f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Importamos un dataset de prueba para poder entrenar el conjunto de datos.\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82522a0-82fc-4902-8825-1d8e9f564f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.001\n",
      "[1,  4000] loss: 0.001\n",
      "[1,  6000] loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    loss_total = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optim1.zero_grad()\n",
    "        #optimizador.zero_grad()\n",
    "\n",
    "        outputs = red_cnn(inputs)\n",
    "        loss = loss_fun(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizador.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch+1}, {i+1:5d}] loss: {loss/2000:.3f}')\n",
    "print(\"Hemos acabado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0817e-dd5b-4601-afff-7a1a49db5fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
